---
title: <뉴스 번역> 학계는 연구를 위한 강력한 칩이 부족하다
date: 2024-11-21
categories: [영어 공부, 뉴스 번역]
tags: [News, Nature]
description: 학계와 산업계의 연구자들이 AI 모델 training을 위해 사용할 수 있는 컴퓨팅 파워의 차이가 상당하다
published: true
---

***

* 학습 목적의 번역이며, 오역이나 의역이 포함될 수 있습니다.

***

* Journal: Nature
* Date: 2024년 11월 20일
* Author: Helena Kudiabor
* Link: <https://www.nature.com/articles/d41586-024-03792-6>

***
<figure align="center">
  <img src="https://media.nature.com/lw767/magazine-assets/d41586-024-03792-6/d41586-024-03792-6_27718124.jpg?as=webp" width="600px" height="450px" alt="">
  <figcaption style="font-size:12px">거대 테크 기업 NVIDIA의 H100 GPU는 AI 연구에서 선호되는 컴퓨터 칩이다. Credit: NVIDIA</figcaption>
  <br/>

<p style="text-align: left">
&ensp;전 세계 수십 개 연구 기관의 학자들을 대상으로 한 설문조사에 따르면, 많은 대학 연구자가 AI 연구를 위해 사용할 수 있는 컴퓨팅 파워가 제한되어 좌절하고 있다고 합니다.<br/><br/>
&ensp;10월 30일 arXiv에 사전 게재된 논문[1]에서 학계에서는 진보된 컴퓨터 시스템을 사용하는 것이 제한된다고 주장했습니다. 이는 대형 언어 모델을 비롯한 AI 연구에 있어 큰 걸림돌이 될 수 있습니다. 특히, 학계에 있는 연구자들은 AI 모델 training에 주로 사용되는 그래픽 처리 장치(GPU)를, 수천만 달러에 달하는 비용의 문제로 충분히 사용할 수 없는 경우도 있었습니다. 반면, 거대 테크 기업의 연구자들은 높은 예산을 배당받아 더 많은 GPU를 사용할 수 있었습니다.<br/><br/>
&ensp;논문의 공저자이자 로드아일랜드 프로비던스의 브라운 대학교에 있는 컴퓨터 공학자인 Apoorv Khandelwal은 "모든 GPU는 연구를 추진할 힘을 제공하지만, 거대 기업들이 수천개의 GPU를 사용할 때, 학계에서는 단지 몇 개만 사용합니다."라고 언급했습니다. 워싱턴 D.C.에 위치한 비영리 AI 연구 기관인 EleutherAI의 전무이사 Stella Biderman은 "학계와 산업계에서 개발된 모델은 차이가 클 수도 있지만, 작을 수도 있습니다. 이 차이에 대해 연구하는 것은 매우 중요합니다."라고 주장하기도 했습니다.<br/>
</p>

<h3 style="text-align: left">기나긴 기다림</h3>

<p style="text-align: left">
&ensp;학계에서 사용할 수 있는 컴퓨팅 자원을 확인하기 위해, Khandelwal과 동료 연구자들은 35개 연구 기관의 50명의 연구자를 대상으로 설문 조사를 진행했습니다. 응답자 중 66%가 그들이 가진 컴퓨팅 파워에 대한 만족도를 5점 만점에 3점 이하로 줄 정도로 만족하지 않는다고 파악되었습니다. 대학에서는 GPU에 대한 접근 권한에 대한 설정이 다양합니다. 어떤 곳은 학부와 학생들이 중앙 컴퓨터를 공유하고 있어 연구자들이 GPU를 사용하겠다고 신청해야 합니다. 어떤 곳은 연구실 구성원들이 바로 사용할 수 있도록 따로 구매하기도 합니다.<br/>
</p>

<figure align="center">
  <img src="https://media.nature.com/lw767/magazine-assets/d41586-024-03792-6/d41586-024-03792-6_27718682.png?as=webp" width="600px" height="450px" alt="">
  <figcaption style="font-size:12px">Source: Ref. 1</figcaption>
  <br/>

<p style="text-align: left">
&ensp;몇몇 연구자들은 GPU를 사용하기 위해 수일을 기다려야 한다고 답변했으며, 마감 기한 언저리까지 기다려야 하는 경우도 있다고 응답하기도 했습니다. 해당 연구에서는 지역적 차이에 대해서도 주목했는데, 한 응답자는 중동에서는 GPU를 구하기 매우 힘들다고 하소연했습니다. 또한, AI 연구를 위해 디자인된 강력한 칩인 NVIDIA의 H100 GPU를 사용한다고 응답한 비율은 10% 정도에 불과했습니다.<br/><br/>
&ensp;이러한 장벽은 LLM처럼 방대한 데이터셋을 pre-training 하는 과정을 어렵게 합니다. Khandelwal은 "Pre-training에 대해 과학계에서는 생각도 하지 않을 정도로 학계에서 감당하기에 너무 비싼 과정입니다."라고 언급했으며, 그와 둉료들은 학계가 AI 연구에 대한 독특한 관점을 제공할 수 있지만, 컴퓨팅 파워에 대한 사용 제한이 이를 어렵게 만든다고 주장했습니다.<br/><br/>
&ensp;연구의 공저자이자, 브라운 대학에서 컴퓨터 공학과 언어학을 연구하는 Ellie Pavlick은 "장기간의 성장과 기술 발전을 위해 학계가 건강하고 경쟁적인 연구 환경을 가지는 것이 매우 중요합니다. 산업계의 연구는 분명 상업적으로 이용될 것이고, 이에 따라 발전이 저해될 수 있습니다."라고 언급했습니다.<br/>
</p>

<h3 style="text-align: left">효과적인 방법</h3>

<p style="text-align: left">
&ensp;연구자들은 학계에서 컴퓨팅 자원을 덜 사용하고도 이를 잘 활용할 수 있는 방법을 조사하기도 했습니다. 1개에서 8개의 GPU를 사용할 때 LLM의 pre-train에 걸리는 시간을 측정했을 때, 컴퓨팅 자원이 제한되어 있음에도 불구하고 많은 모델을 성공적으로 만들 수 있었지만, 시간이 오래 걸리고 더 효율적인 방법을 채택해야 했습니다. Khandelwal은 자신들이 GPU를 더 오래 사용함으로써 산업계가 가진 컴퓨팅 파워와의 차이를 어느 정도는 메꿀 수 있었다고 표현했습니다.<br/><br/>
&ensp;독일 자르브뤼켄의 자를란트 대학교에서 neuroexplicit model (신경학과 사람이 해석할 수 있는 요소를 접목해 만든 모델)을 연구하는 Ji-Ung Lee는 "제한된 컴퓨팅 자원을 사용해 예상보다 큰 모델을 training 시킬 수 있다는 것은 멋진 일입니다."라고 언급했습니다. 또한 추후 연구에서 컴퓨팅 자원에 접근하기 힘든 중소기업 연구자들의 경험에 대해 조사하는 것도 좋을 것이라고 덧붙였으며, "제한 없이 컴퓨팅 파워를 사용할 수 있는 권한이 있어도, 항상 제한 없이 컴퓨팅 파워를 사용할 수 있는 것은 아닙니다."라고도 언급했습니다.<br/>
<br/>
[1] Khandelwal, A. et al. Preprint at arXiv https://doi.org/10.48550/arXiv.2410.23261 (2024).
</p>
